{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Youtube Scraper (using keyword/url).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO31tbrxTsQya9pfU2kDQ4D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgabelarde/youtube-scrape/blob/main/Youtube_Scraper_(using_keyword_url).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QNSOvlvRZW"
      },
      "source": [
        "# **YOUTUBE WEBSCRAPING**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu6SRqj06ECa"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        " <br>\n",
        "\n",
        "# **PRE-SCRAPING**\n",
        "\n",
        "> Run these code before executing scraping below\n",
        "\n",
        "<br>\n",
        "\n",
        "##**Pre-requisite**##\n",
        "\n",
        ">1. Excel file with a column of **either video urls (jump to 2)/keywords (jump to 3)**\n",
        "2. Your excel file must be named \"youtube_urls.xlsx\"\n",
        "3. Your excel file must be named \"youtube_keywords.xlsx\"\n",
        "\n",
        "***Note: Please remember to run the code in order***\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE4T7I46t60M"
      },
      "source": [
        "## **Installing and importing required packages for scraping**\n",
        ">You MUST run this to successfully scrape data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zgPx3pg3tU6",
        "outputId": "7d4fcdce-60c6-4fa1-96d1-6f798ca72d7f"
      },
      "source": [
        "# Installation of external packages selenium and youtube_transcript_api packages\n",
        "\n",
        "!apt-get update # to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "# import sys\n",
        "# sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "!pip install selenium\n",
        "\n",
        "!pip install youtube_transcript_api\n",
        "!pip install youtube-search-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [61.8 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,185 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,415 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [450 kB]\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [800 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,772 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,184 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [481 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [906 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.5 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,619 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.6 kB]\n",
            "Fetched 13.3 MB in 4s (3,066 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 86.0 MB of archives.\n",
            "After this operation, 298 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 91.0.4472.77-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 91.0.4472.77-0ubuntu0.18.04.1 [76.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 91.0.4472.77-0ubuntu0.18.04.1 [3,948 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 91.0.4472.77-0ubuntu0.18.04.1 [4,838 kB]\n",
            "Fetched 86.0 MB in 4s (21.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_91.0.4472.77-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/24/581920922d80f6d6d2feb30787aa4d9e298db83cb081c1de1a63cc7cf121/youtube_transcript_api-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube_transcript_api) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2020.12.5)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.4.1\n",
            "Collecting youtube-search-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/4c/92dd7b3b29afcab9ca968853d63bcec4638bba2183d3665c3ef1a5efbc43/youtube_search_python-1.4.5-py3-none-any.whl (69kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 3.2MB/s \n",
            "\u001b[?25hCollecting httpx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/69/17b725b1bfe63228369cddf50206381eadb14bc46b933a864ae93ea1b9bf/httpx-0.18.1-py3-none-any.whl (75kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 3.8MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting httpcore<0.14.0,>=0.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/4c/90b46b4e3121a48cc0dcebb665d745937c75b0d691f54d351a3b3646e620/httpcore-0.13.4-py3-none-any.whl (58kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.1MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->youtube-search-python) (2020.12.5)\n",
            "Requirement already satisfied: idna; extra == \"idna2008\" in /usr/local/lib/python3.7/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx->youtube-search-python) (2.10)\n",
            "Collecting anyio==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/47/0c30d51c30d23eecead360e8624ad3972ebd8f9933265a91fca1372d37cf/anyio-3.1.0-py3-none-any.whl (74kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.9MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.14.0,>=0.13.0->httpx->youtube-search-python) (3.7.4.3)\n",
            "Installing collected packages: rfc3986, sniffio, anyio, h11, httpcore, httpx, youtube-search-python\n",
            "Successfully installed anyio-3.1.0 h11-0.12.0 httpcore-0.13.4 httpx-0.18.1 rfc3986-1.5.0 sniffio-1.2.0 youtube-search-python-1.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzs1N2v36Py"
      },
      "source": [
        "# Import required packages\n",
        "# Basic Python Packages\n",
        "import asyncio\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from urllib.request import urlopen\n",
        "import os\n",
        "\n",
        "# Google package to upload file\n",
        "from google.colab import files\n",
        "\n",
        "# External Packages\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver  # for webdriver\n",
        "from selenium.common import exceptions\n",
        "from selenium.webdriver.chrome.options import \\\n",
        "    Options  # for suppressing the browser\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtubesearchpython import VideosSearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwd4WachuSkT"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Information Scraping**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFGf9YVmu47v"
      },
      "source": [
        ">Initial scrapping method to scrap from keyword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWO9DvOv4gxM"
      },
      "source": [
        "# Credits to @Jalexmercerind from GitHub for youtube-search-python api\n",
        "# https://github.com/alexmercerind/youtube-search-python\n",
        "\n",
        "def scrape_keyword_search(keyword, limit):\n",
        "    # Scrape data\n",
        "    videosSearch = VideosSearch(keyword, limit=limit)\n",
        "    search_res = videosSearch.result()['result']\n",
        "\n",
        "    # Convert dictionary results to dataframe\n",
        "    res_df = pd.DataFrame(search_res)\n",
        "\n",
        "    # Add a column that indicates keyword\n",
        "    res_df['keyword'] = keyword\n",
        "\n",
        "    # Delete some empty/unneeded columns\n",
        "    # publishedTime replaced in other method for accuracy\n",
        "    # viewCount, link replaced in another method for ease\n",
        "    # description replaced in other method for completeness\n",
        "    res_df.drop([\n",
        "        'id', 'shelfTitle', 'accessibility', 'richThumbnail', 'publishedTime',\n",
        "        'viewCount', 'descriptionSnippet', 'title'\n",
        "    ],\n",
        "      axis=1,\n",
        "      inplace=True)\n",
        "    \n",
        "    return res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBkQ0iLvEPX"
      },
      "source": [
        "<br>\n",
        "\n",
        "Scraping method helper to gather extra video information;\n",
        ">- Used after scraping search results when scraping keyword data\n",
        "- Used as method to scrape data from url\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ81OjE64RIQ"
      },
      "source": [
        "# Credits to @faheel from GitHub for youtube-scraper-api\n",
        "# https://github.com/faheel/youtube-scraper-api\n",
        "\n",
        "def scrape_video_info(url):\n",
        "    '''\n",
        "    Scrapes data from the YouTube video's page whose ID is passed in the URL,\n",
        "    and returns a JSON object as a response.\n",
        "    '''\n",
        "\n",
        "    # Set default values in case nothing is returned\n",
        "    RESPONSE = {\n",
        "        'id': \"\",\n",
        "        'title': \"\",\n",
        "        'url': url,\n",
        "        'upload_date': \"\",\n",
        "        'description': \"\",\n",
        "        'genre': \"\",\n",
        "        'user_id': \"\",\n",
        "        # 'user_name': \"\",\n",
        "        # 'is_user_verified': False,\n",
        "        'views': 0,\n",
        "        'likes': 0,\n",
        "        'dislikes': 0,\n",
        "        'is_paid': False,\n",
        "        'is_unlisted': False,\n",
        "        'is_family_friendly': True\n",
        "    }\n",
        "    \n",
        "\n",
        "    # Helper functions\n",
        "    def is_true(string):\n",
        "        return string.lower() not in ['false', '0']\n",
        "\n",
        "    def make_soup(url):\n",
        "      '''\n",
        "        Reads the contents at the given URL and returns a Python object based on\n",
        "        the structure of the contents (HTML).\n",
        "      '''\n",
        "      html = urlopen(url).read()\n",
        "      return BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    soup = make_soup(url)\n",
        "\n",
        "    # video_containers = soup.find_all('div', class_ = 'watch-main-col')\n",
        "    soup_itemprop = soup.find(id='watch7-content')\n",
        "\n",
        "    if len(soup_itemprop.text) > 1:\n",
        "        video = RESPONSE\n",
        "\n",
        "        # get data from tags having `itemprop` attribute\n",
        "        for tag in soup_itemprop.find_all(itemprop=True, recursive=False):\n",
        "            key = tag['itemprop']\n",
        "            if key == 'videoId':\n",
        "                # get video ID\n",
        "                video['id'] = tag['content']\n",
        "            elif key == 'name':\n",
        "                # get video's name\n",
        "                video['title'] = tag['content']\n",
        "            elif key == 'datePublished':\n",
        "                # get video's upload date\n",
        "                video['upload_date'] = tag['content']\n",
        "            elif key == 'genre':\n",
        "                # get video's genre (category)\n",
        "                video['genre'] = tag['content']\n",
        "            elif key == 'paid':\n",
        "                # is the video paid?\n",
        "                video['is_paid'] = is_true(tag['content'])\n",
        "            elif key == 'unlisted':\n",
        "                # is the video unlisted?\n",
        "                video['is_unlisted'] = is_true(tag['content'])\n",
        "            elif key == 'isFamilyFriendly':\n",
        "                # is the video family friendly?\n",
        "                video['is_family_friendly'] = is_true(tag['content'])\n",
        "            elif key == 'interactionCount':\n",
        "                # get video's views\n",
        "                video['views'] = int(tag['content'])\n",
        "            elif key == 'channelId':\n",
        "                # get uploader's channel ID\n",
        "                video['user_id'] = tag['content']\n",
        "            # elif key == 'author':\n",
        "            #     # get uploader's channel name\n",
        "            #     video['user_name'] = tag['content']\n",
        "            elif key == 'description':\n",
        "                video['description'] = tag['content']\n",
        "\n",
        "        all_scripts = soup.find_all('script')\n",
        "        for i in range(len(all_scripts)):\n",
        "            try:\n",
        "                if 'ytInitialData' in all_scripts[i].string:\n",
        "                    match = re.findall(\n",
        "                        \"label(.*)\",\n",
        "                        re.findall(\"LIKE(.*?)like\",\n",
        "                                   all_scripts[i].string)[0])[0]\n",
        "                    hasil = (''.join(match.split(',')).split(\"\\\"\")[-1]).strip()\n",
        "                    try:\n",
        "                        video['likes'] = eval(hasil)\n",
        "                    except:\n",
        "                        video['likes'] = 0\n",
        "\n",
        "                    match = re.findall(\n",
        "                        \"label(.*)\",\n",
        "                        re.findall(\"DISLIKE(.*?)dislike\",\n",
        "                                   all_scripts[i].string)[0])[0]\n",
        "                    hasil = (''.join(match.split(',')).split(\"\\\"\")[-1]).strip()\n",
        "                    try:\n",
        "                        video['dislikes'] = eval(hasil)\n",
        "                    except:\n",
        "                        video['dislikes'] = 0\n",
        "                  \n",
        "            except:\n",
        "                pass\n",
        "        return RESPONSE\n",
        "\n",
        "    return ({'error': 'Video with the ID {} does not exist'.format(id)})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWP_1cFbtuln"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Comments Scraping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amFL6jm02TJ_"
      },
      "source": [
        "# Amended code from @dddat1017\n",
        "# https://github.com/dddat1017/Scraping-Youtube-Comments\n",
        "\n",
        "# Method to scrape youtube comments\n",
        "# Output: dataframe\n",
        "def scrape_comments(url):\n",
        "    '''\n",
        "    Extracts the comments from the Youtube video given by the URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to the Youtube video\n",
        "\n",
        "    Raises:\n",
        "        selenium.common.exceptions.NoSuchElementException:\n",
        "        When certain elements to look for cannot be found\n",
        "    '''\n",
        "\n",
        "    # Note: Download and replace argument with path to the driver executable.\n",
        "    # Simply download the executable and move it into the webdrivers folder.\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "\n",
        "    # Navigates to the URL, maximizes the current window, and\n",
        "    # then suspends execution for (at least) 5 seconds (this\n",
        "    # gives time for the page to load).\n",
        "    driver.get(url)\n",
        "    time.sleep(5)\n",
        "\n",
        "    try:\n",
        "        # Extract the elements storing the video title and\n",
        "        # comment section.\n",
        "        # title = driver.find_element_by_xpath('//*[@id='container']/h1/yt-formatted-string').text\n",
        "        comment_section = driver.find_element_by_xpath('//*[@id=\"comments\"]')\n",
        "    except exceptions.NoSuchElementException:\n",
        "        # Note: Youtube may have changed their HTML layouts for\n",
        "        # videos, so raise an error for sanity sake in case the\n",
        "        # elements provided cannot be found anymore.\n",
        "        error = 'Error: Double check selector OR '\n",
        "        error += 'element may not yet be on the screen at the time of the find operation'\n",
        "        print(error)\n",
        "\n",
        "    # Scroll into view the comment section, then allow some time\n",
        "    # for everything to be loaded as necessary.\n",
        "    driver.execute_script('arguments[0].scrollIntoView();', comment_section)\n",
        "    time.sleep(7)\n",
        "\n",
        "    # Scroll all the way down to the bottom in order to get all the\n",
        "    # elements loaded (since Youtube dynamically loads them).\n",
        "    last_height = driver.execute_script(\n",
        "        'return document.documentElement.scrollHeight')\n",
        "\n",
        "    while True:\n",
        "        # Scroll down 'til 'next load'.\n",
        "        driver.execute_script(\n",
        "            'window.scrollTo(0, document.documentElement.scrollHeight);')\n",
        "\n",
        "        # Wait to load everything thus far.\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Calculate new scroll height and compare with last scroll height.\n",
        "        new_height = driver.execute_script(\n",
        "            'return document.documentElement.scrollHeight')\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    # One last scroll just in case.\n",
        "    driver.execute_script(\n",
        "        'window.scrollTo(0, document.documentElement.scrollHeight);')\n",
        "\n",
        "    try:\n",
        "        # Extract the elements storing the usernames and comments.\n",
        "        username_elems = driver.find_elements_by_xpath(\n",
        "            '//*[@id=\"author-text\"]')\n",
        "        comment_elems = driver.find_elements_by_xpath(\n",
        "            '//*[@id=\"content-text\"]')\n",
        "    except exceptions.NoSuchElementException:\n",
        "        error = 'Error: Double check selector OR '\n",
        "        error += 'element may not yet be on the screen at the time of the find operation'\n",
        "        print(error)\n",
        "\n",
        "    comments_df = pd.DataFrame({\n",
        "        'commenter_username': [username.text for username in username_elems],\n",
        "        'comment_text': [comment.text for comment in comment_elems],\n",
        "    })\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "    return comments_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDx8xzp8uMIM"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Transcript Scraping**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwORQ2EisPl1"
      },
      "source": [
        "def scrape_transcript(url):\n",
        "    video_id = url.split('watch?v=')[1]\n",
        "    try:\n",
        "        transcripts = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "        transcript_df = pd.DataFrame({\n",
        "            'transcribed_text':\n",
        "            [transcripts['text'] for transcripts in transcripts],\n",
        "            'start_time': [transcripts['start'] for transcripts in transcripts],\n",
        "            'duration': [transcripts['duration'] for transcripts in transcripts],\n",
        "        })\n",
        "        \n",
        "        return transcript_df\n",
        "    except:\n",
        "      return \"No transcriptions for this video could be found\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBS5EuMo-jv-"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Helper for Comments and Transcriptions Scraping**\n",
        "\n",
        ">- Helps create an outputs folder\n",
        "-  Calls both scraping functions\n",
        "- Saves output as excel file with different sheets for comments and transcript"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGdXX0b956a2"
      },
      "source": [
        "# Create folder to store all comments and transcript files\n",
        "\n",
        "output_path = 'comments and transcript outputs'\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFICDovjx9hw"
      },
      "source": [
        "def scrape_comments_transcript(url, title):\n",
        "  comments_df = scrape_comments(url)\n",
        "  transcript_df = scrape_transcript(url)\n",
        "  \n",
        "  writer = pd.ExcelWriter(os.path.join(output_path, \n",
        "                          title + \".xlsx\"), engine='openpyxl')\n",
        "  \n",
        "  if isinstance(comments_df, pd.DataFrame): \n",
        "    comments_df.to_excel(writer, sheet_name='comments', index=False)\n",
        "  if isinstance(transcript_df, pd.DataFrame):\n",
        "    transcript_df.to_excel(writer, sheet_name='transcript', index=False)\n",
        "\n",
        "  writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsieZOYvf4-"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Final Cleaning process**\n",
        ">Used after keyword scrapping process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZGE70FChS1m"
      },
      "source": [
        "def clean_keywords_data(df):\n",
        "    # NTS: can use json_normalize because the dictionaries are flat\n",
        "    # then drop initial clustered (dictionary-ed) column\n",
        "    # Expand 'more_info' column that is currently a dictionary of data into columns\n",
        "    df = df.join(pd.json_normalize(df.more_info)).drop('more_info', axis=1)\n",
        "      \n",
        "\n",
        "    # Expand 'video_creator' column that is currently a dictionary of data into columns\n",
        "    df = df.join(pd.json_normalize(df.channel)['name']).drop(\n",
        "        'channel', axis=1)\n",
        "    \n",
        "    # Rename columns accordingly for literacy\n",
        "    df.rename(\n",
        "        columns={\n",
        "            'thumbnails': 'thumbnail_urls',\n",
        "            'name': 'user_name'\n",
        "        },\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    # Delete empty/unneeded columns\n",
        "    # link replaced in another method for ease\n",
        "    df.drop(['link'], axis=1, inplace=True)\n",
        "\n",
        "    df['thumbnail_urls'] = df['thumbnail_urls'].apply(\n",
        "        lambda lst: ', \\n'.join([item['url'] for item in lst]))\n",
        "\n",
        "    # Rearrange columns for readability\n",
        "    df = df[[\n",
        "        'type', 'id', 'title', 'url', 'thumbnail_urls','keyword', 'description',\n",
        "        'duration', 'upload_date', 'genre', 'user_id', 'user_name',\n",
        "        'views', 'likes', 'dislikes', 'is_paid', 'is_unlisted', 'is_family_friendly'\n",
        "    ]]\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvH7xQt78IiC"
      },
      "source": [
        "\n",
        " <br> <br>\n",
        "\n",
        "\n",
        "---\n",
        " <br>\n",
        "\n",
        "# **KEYWORDS SCRAPING**\n",
        ">Run this only if you are doing keyword scraping!\n",
        "\n",
        "Note: Remember that your file name has to be named '**youtube_keywords.xlsx**'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYRuUE1T_z2L"
      },
      "source": [
        "### Video data scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "IegoMyjY4qP_",
        "outputId": "7a90db52-71a0-402e-edef-41be3b97f9a4"
      },
      "source": [
        "# Upload excel file here\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7e97c27-d670-453f-863a-9c1c0186fc3f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7e97c27-d670-453f-863a-9c1c0186fc3f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving youtube_keywords.xlsx to youtube_keywords.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLZ7n47A4_jw",
        "outputId": "4e7e6748-e65d-4fbd-9e13-da31f834e6ce"
      },
      "source": [
        "# Read in your file to a dataframe\n",
        "keywords_df = pd.read_excel(uploaded.get('youtube_keywords.xlsx'))\n",
        "\n",
        "# Enter the number of results you wish to fetch:\n",
        "search_limit = int(input(\"How much data would you like to fetch per keyword? \\n Please only enter an integer.\\n\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How much data would you like to fetch per keyword? \n",
            " Please only enter an integer.\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3NkN7e_BL0"
      },
      "source": [
        "# Convert keywords into a list\n",
        "keywords = list(keywords_df['keyword'])\n",
        "\n",
        "# Map each keyword to function that will scrape it's search results\n",
        "df_lst = [scrape_keyword_search(keyword, search_limit) for keyword in keywords]\n",
        "\n",
        "# Join scraped data from each keyword scraped\n",
        "keywords_res_df = pd.concat(df_lst, ignore_index=True)\n",
        "\n",
        "# Scrape extra information about each video\n",
        "keywords_res_df['more_info'] = keywords_res_df['link'].apply(scrape_video_info)\n",
        "\n",
        "# Clean all the data we scraped\n",
        "keywords_res_df = clean_keywords_data(keywords_res_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "9rJxO36TDtmD",
        "outputId": "082d3706-9392-49c1-e5a1-1093103b4afe"
      },
      "source": [
        "# Preview data to check\n",
        "keywords_res_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>thumbnail_urls</th>\n",
              "      <th>keyword</th>\n",
              "      <th>description</th>\n",
              "      <th>duration</th>\n",
              "      <th>upload_date</th>\n",
              "      <th>genre</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_name</th>\n",
              "      <th>views</th>\n",
              "      <th>likes</th>\n",
              "      <th>dislikes</th>\n",
              "      <th>is_paid</th>\n",
              "      <th>is_unlisted</th>\n",
              "      <th>is_family_friendly</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>video</td>\n",
              "      <td>1HygThMLzGs</td>\n",
              "      <td>ü§£Funny Dog Videos 2020ü§£ üê∂ It's time to LAUGH w...</td>\n",
              "      <td>https://www.youtube.com/watch?v=1HygThMLzGs</td>\n",
              "      <td>https://i.ytimg.com/vi/1HygThMLzGs/hq720.jpg?s...</td>\n",
              "      <td>dog</td>\n",
              "      <td>ü§£Funny Dog Videos 2020ü§£ üê∂ It's time to LAUGH w...</td>\n",
              "      <td>9:56</td>\n",
              "      <td>2020-05-05</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "      <td>UCIqWFhsJm2VU1ZuGrZnx3Pw</td>\n",
              "      <td>MAI PM</td>\n",
              "      <td>28928713</td>\n",
              "      <td>349782</td>\n",
              "      <td>15106</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>video</td>\n",
              "      <td>ewJyUcL91oo</td>\n",
              "      <td>Wasabi, the Pekingese, crowned Best in Show at...</td>\n",
              "      <td>https://www.youtube.com/watch?v=ewJyUcL91oo</td>\n",
              "      <td>https://i.ytimg.com/vi/ewJyUcL91oo/hq720.jpg?s...</td>\n",
              "      <td>dog</td>\n",
              "      <td>A champion was crowned at the 145th Westminste...</td>\n",
              "      <td>8:32</td>\n",
              "      <td>2021-06-13</td>\n",
              "      <td>Sports</td>\n",
              "      <td>UCwNqHDsnBCKT-olwJwIFyfg</td>\n",
              "      <td>FOX Sports</td>\n",
              "      <td>9977</td>\n",
              "      <td>132</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>video</td>\n",
              "      <td>gpyB54lSpYg</td>\n",
              "      <td>Dog Reviews Food With Baby Puppy | Tucker Tast...</td>\n",
              "      <td>https://www.youtube.com/watch?v=gpyB54lSpYg</td>\n",
              "      <td>https://i.ytimg.com/vi/gpyB54lSpYg/hq720.jpg?s...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Dog Reviews Food With Baby Puppy | Tucker Tast...</td>\n",
              "      <td>3:11</td>\n",
              "      <td>2020-12-13</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "      <td>UCNSzfesc7IgWZwg4n6uXr1A</td>\n",
              "      <td>Tucker Budzyn</td>\n",
              "      <td>25020293</td>\n",
              "      <td>468935</td>\n",
              "      <td>16841</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>video</td>\n",
              "      <td>spjSnOuOxUw</td>\n",
              "      <td>TV for Dogs! Entertainment for Dogs and Puppie...</td>\n",
              "      <td>https://www.youtube.com/watch?v=spjSnOuOxUw</td>\n",
              "      <td>https://i.ytimg.com/vi/spjSnOuOxUw/hqdefault_l...</td>\n",
              "      <td>dog</td>\n",
              "      <td>TV for Dogs! Entertainment for Dogs and Puppie...</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-03-22</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "      <td>UCVqytdh_eTzWlh1K8Yst_rg</td>\n",
              "      <td>Calm Your Dog - Relaxing Music and TV for Dogs</td>\n",
              "      <td>307027</td>\n",
              "      <td>935</td>\n",
              "      <td>63</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>video</td>\n",
              "      <td>eX2qFMC8cFo</td>\n",
              "      <td>Funniest Cats üòπ - Don't try to hold back Laugh...</td>\n",
              "      <td>https://www.youtube.com/watch?v=eX2qFMC8cFo</td>\n",
              "      <td>https://i.ytimg.com/vi/eX2qFMC8cFo/hq720.jpg?s...</td>\n",
              "      <td>cat</td>\n",
              "      <td>Funniest Cats üòπ - Don't try to hold back Laugh...</td>\n",
              "      <td>9:10</td>\n",
              "      <td>2020-10-29</td>\n",
              "      <td>Pets &amp; Animals</td>\n",
              "      <td>UCYPrd7A27nLhQONcCIfFTaA</td>\n",
              "      <td>Funny Cats Life</td>\n",
              "      <td>47204786</td>\n",
              "      <td>761635</td>\n",
              "      <td>40301</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    type           id  ... is_unlisted is_family_friendly\n",
              "0  video  1HygThMLzGs  ...       False               True\n",
              "1  video  ewJyUcL91oo  ...       False               True\n",
              "2  video  gpyB54lSpYg  ...       False               True\n",
              "3  video  spjSnOuOxUw  ...       False               True\n",
              "4  video  eX2qFMC8cFo  ...       False               True\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REOtryjq73Fp"
      },
      "source": [
        "# export data to excel sheet\n",
        "keywords_res_df.to_excel(r'youtube_keywords_output.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nTcUykV_w7o"
      },
      "source": [
        "### Video comments + transcript scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "u2TXSiiH_w7p",
        "outputId": "d195c48f-f03b-4024-a02a-2df5f507872b"
      },
      "source": [
        "keywords_res_df.apply(lambda x: scrape_comments_transcript(x['url'], x['title']), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-131a4ef031c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords_res_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscrape_comments_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-131a4ef031c6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords_res_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscrape_comments_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-55040c66f04d>\u001b[0m in \u001b[0;36mscrape_comments_transcript\u001b[0;34m(url, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_comments_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcomments_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtranscript_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   writer = pd.ExcelWriter(os.path.join(output_path, \n",
            "\u001b[0;32m<ipython-input-17-1d1e59d1261b>\u001b[0m in \u001b[0;36mscrape_comments\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Wait to load everything thus far.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Calculate new scroll height and compare with last scroll height.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-us-YlxQ8YUd"
      },
      "source": [
        " <br>\n",
        "\n",
        "**end of keyword scraping**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        " <br>\n",
        "\n",
        "# **URLS SCRAPING**\n",
        ">Run this only if you are doing url scraping!\n",
        "\n",
        "Note: Remember that your file name has to be named '**youtube_urls.xlsx**'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBQX5trVunLq"
      },
      "source": [
        "### Video data scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ON5m_y9Izvc7",
        "outputId": "8ab5a467-fe3f-482c-96f2-1e4a2bf8351c"
      },
      "source": [
        "# Upload excel file here\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3021e9ad-6428-4ba1-b3d9-ebbc0b6f3bb0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3021e9ad-6428-4ba1-b3d9-ebbc0b6f3bb0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving youtube_urls.xlsx to youtube_urls.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvSxJiHVzZeo"
      },
      "source": [
        "# Read in your file to a dataframe\n",
        "urls_df = pd.read_excel(uploaded.get('youtube_urls.xlsx'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWwrX4OuzZeq"
      },
      "source": [
        "# Convert keywords into a list\n",
        "urls = list(urls_df['URL'])\n",
        "\n",
        "# Map each url to function that will scrape its video's data\n",
        "urls_res_df = pd.DataFrame(map(scrape_video_info, urls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "XNWg34LdD_SJ",
        "outputId": "923f451d-e2c8-4ac3-b2e1-a5129b27748c"
      },
      "source": [
        "# Preview data to check\n",
        "urls_res_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>upload_date</th>\n",
              "      <th>description</th>\n",
              "      <th>genre</th>\n",
              "      <th>user_id</th>\n",
              "      <th>views</th>\n",
              "      <th>likes</th>\n",
              "      <th>dislikes</th>\n",
              "      <th>is_paid</th>\n",
              "      <th>is_unlisted</th>\n",
              "      <th>is_family_friendly</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ab2hF-EN1lw</td>\n",
              "      <td>Jake Scott - Anymore (Audio)</td>\n",
              "      <td>https://www.youtube.com/watch?v=Ab2hF-EN1lw</td>\n",
              "      <td>2021-06-10</td>\n",
              "      <td>Stream \"Anymore\" here: http://smarturl.it/jake...</td>\n",
              "      <td>Music</td>\n",
              "      <td>UCDvr1r2d2Ma4GrP8rKP8HjA</td>\n",
              "      <td>25702</td>\n",
              "      <td>2487</td>\n",
              "      <td>10</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kz2MLCCdxsM</td>\n",
              "      <td>Jake Scott - Off (Lyric Video)</td>\n",
              "      <td>https://www.youtube.com/watch?v=Kz2MLCCdxsM</td>\n",
              "      <td>2021-04-30</td>\n",
              "      <td>Stream \"Off\" here: http://smarturl.it/Offjakes...</td>\n",
              "      <td>Music</td>\n",
              "      <td>UCDvr1r2d2Ma4GrP8rKP8HjA</td>\n",
              "      <td>162597</td>\n",
              "      <td>8464</td>\n",
              "      <td>34</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zz8lKer83Ac</td>\n",
              "      <td>Jake Scott &amp; Josie Dunne - \"CWJBHN\" (Lyric Video)</td>\n",
              "      <td>https://www.youtube.com/watch?v=zz8lKer83Ac</td>\n",
              "      <td>2020-07-16</td>\n",
              "      <td>Jake Scott music, merch &amp; more: https://solo.t...</td>\n",
              "      <td>Music</td>\n",
              "      <td>UCDvr1r2d2Ma4GrP8rKP8HjA</td>\n",
              "      <td>2379153</td>\n",
              "      <td>94933</td>\n",
              "      <td>435</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HaBOp-9pDDQ</td>\n",
              "      <td>Jake Scott - Meet My Shadow (Audio Video)</td>\n",
              "      <td>https://www.youtube.com/watch?v=HaBOp-9pDDQ</td>\n",
              "      <td>2021-05-20</td>\n",
              "      <td>Stream \"Meet My Shadow\" here: http://smarturl....</td>\n",
              "      <td>Music</td>\n",
              "      <td>UCDvr1r2d2Ma4GrP8rKP8HjA</td>\n",
              "      <td>45579</td>\n",
              "      <td>2762</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ... is_family_friendly\n",
              "0  Ab2hF-EN1lw  ...               True\n",
              "1  Kz2MLCCdxsM  ...               True\n",
              "2  zz8lKer83Ac  ...               True\n",
              "3  HaBOp-9pDDQ  ...               True\n",
              "\n",
              "[4 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC0tMtpJ7YTi"
      },
      "source": [
        "# export data to excel sheet\n",
        "urls_res_df.to_excel(r'youtube_urls_output.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC-q6cN1v7mD"
      },
      "source": [
        "### Video comments + transcript scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Kd4a-oerebp",
        "outputId": "102eb59e-7e79-4923-847b-f246647d8bfc"
      },
      "source": [
        "urls_res_df.apply(lambda x: scrape_comments_transcript(x['url'], x['title']), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    None\n",
              "1    None\n",
              "2    None\n",
              "3    None\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qep9p6CZ5fvO"
      },
      "source": [
        " <br>\n",
        "\n",
        "**end of url scraping**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>"
      ]
    }
  ]
}